[2m2025-11-11T00:51:34.694836Z[0m [32m INFO[0m [2mxdl[0m[2m:[0m Executing file: examples/ml_reg_simple_test.xdl
Regularization Layers Test
===========================

TEST 1: Batch Normalization
Input: [1.000000, 2.000000, 3.000000, 4.000000, 5.000000, 6.000000, 7.000000, 8.000000, 9.000000, 10.000000]
Normalized: [-1.566698, -1.218543, -0.870388, -0.522233, -0.174078, 0.174078, 0.522233, 0.870388, 1.218543, 1.566698]
Mean: -0.000000000000000 , Std: 1.054091914546069
Batch Normalization: PASS

TEST 2: Batch Norm with Gamma/Beta
Scaled (gamma=2, beta=1): [-2.133396, -1.437086, -0.740776, -0.044465, 0.651845, 1.348155, 2.044465, 2.740776, 3.437086, 4.133396]
Mean: 1.000000000000000 , Std: 2.108183829092138
Scaled Batch Norm: PASS

TEST 3: Dropout (Training)
Input: [1.000000, 2.000000, 3.000000, 4.000000, 5.000000, 6.000000, 7.000000, 8.000000, 9.000000, 10.000000]
Dropped (50% rate): [2.000000, 4.000000, 0.000000, 0.000000, 0.000000, 12.000000, 0.000000, 16.000000, 18.000000, 20.000000]
Dropout Training: PASS

TEST 4: Dropout (Inference)
Input: [1.000000, 2.000000, 3.000000, 4.000000, 5.000000, 6.000000, 7.000000, 8.000000, 9.000000, 10.000000]
Output (no dropout): [1.000000, 2.000000, 3.000000, 4.000000, 5.000000, 6.000000, 7.000000, 8.000000, 9.000000, 10.000000]
Dropout Inference: PASS

TEST 5: Inverted Dropout Scaling
Original sum: 55.000000000000000
Dropped sum: 72.000000000000000
Ratio: 1.309090909090909
Inverted Dropout: PASS

===========================
All tests PASSED!
