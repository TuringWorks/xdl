; ============================================================================
; Regularization Layers Test Script
; ============================================================================
; Tests Batch Normalization and Dropout layers

PRINT, '========================================='
PRINT, 'Regularization Layers Test'
PRINT, '========================================='
PRINT, ''

; ============================================================================
; TEST 1: Batch Normalization - Training Mode
; ============================================================================
PRINT, '--- TEST 1: Batch Normalization (Training) ---'

; Create sample activations
activations = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]
PRINT, 'Input activations:', activations

; Apply batch normalization (training mode)
; gamma=1.0, beta=0.0, mode=0 (training)
normalized = XDLML_BATCHNORMALIZATION(activations, 1.0, 0.0, 0)

PRINT, 'Normalized (training):', normalized

; Calculate expected mean and std
expected_mean = MEAN(activations)
expected_std = STDDEV(activations)
PRINT, 'Original mean:', expected_mean
PRINT, 'Original stddev:', expected_std

; Normalized output should have mean~0, std~1
norm_mean = MEAN(normalized)
norm_std = STDDEV(normalized)
PRINT, 'Normalized mean:', norm_mean
PRINT, 'Normalized stddev:', norm_std

IF ABS(norm_mean) LT 0.000001 AND ABS(norm_std - 1.0) LT 0.1 THEN BEGIN
    PRINT, 'Batch Normalization (training): PASS ✓'
END ELSE BEGIN
    PRINT, 'Batch Normalization (training): FAIL ✗'
END
endif
PRINT, ''

; ============================================================================
; TEST 2: Batch Normalization - Inference Mode
; ============================================================================
PRINT, '--- TEST 2: Batch Normalization (Inference) ---'

; Use different running statistics
running_mean = 5.0
running_var = 4.0
test_input = [6.0, 7.0, 8.0]

; Apply batch norm in inference mode (mode=1)
normalized_inf = XDLML_BATCHNORMALIZATION(test_input, 1.0, 0.0, 1, running_mean, running_var)

PRINT, 'Test input:', test_input
PRINT, 'Running mean:', running_mean, ', Running var:', running_var
PRINT, 'Normalized (inference):', normalized_inf
PRINT, 'Batch Normalization (inference): PASS ✓'
PRINT, ''

; ============================================================================
; TEST 3: Batch Normalization with Learned Parameters
; ============================================================================
PRINT, '--- TEST 3: Batch Norm with Gamma/Beta ---'

; Apply with learned scale (gamma=2.0) and shift (beta=1.0)
gamma = 2.0
beta = 1.0
scaled_norm = XDLML_BATCHNORMALIZATION(activations, gamma, beta, 0)

PRINT, 'Input:', activations
PRINT, 'Gamma:', gamma, ', Beta:', beta
PRINT, 'Scaled & shifted:', scaled_norm

; Output should be scaled by gamma and shifted by beta
; After normalization: mean~beta, std~gamma
scaled_mean = MEAN(scaled_norm)
scaled_std = STDDEV(scaled_norm)
PRINT, 'Output mean:', scaled_mean, ' (expected ~', beta, ')'
PRINT, 'Output stddev:', scaled_std, ' (expected ~', gamma, ')'

IF ABS(scaled_mean - beta) LT 0.1 AND ABS(scaled_std - gamma) LT 0.2 THEN BEGIN
    PRINT, 'Batch Norm with parameters: PASS ✓'
END ELSE BEGIN
    PRINT, 'Batch Norm with parameters: FAIL ✗'
END
endif
PRINT, ''

; ============================================================================
; TEST 4: Dropout - Training Mode
; ============================================================================
PRINT, '--- TEST 4: Dropout (Training) ---'

; Create input
input_vals = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]
PRINT, 'Input:', input_vals

; Apply 50% dropout in training mode
dropout_rate = 0.5
dropped = XDLML_DROPOUT(input_vals, dropout_rate, 1, 42)

PRINT, 'Dropout rate:', dropout_rate
PRINT, 'Output (training):', dropped

; Count zeros (dropped units)
n_dropped = 0
FOR i = 0, N_ELEMENTS(dropped)-1 DO BEGIN
    IF dropped[i] EQ 0.0 THEN n_dropped = n_dropped + 1
END
endfor

PRINT, 'Dropped units:', n_dropped, '/', N_ELEMENTS(dropped)

; With 50% dropout, expect around half to be dropped
; (with fixed seed, might vary but should be reasonable)
IF n_dropped GE 3 AND n_dropped LE 7 THEN BEGIN
    PRINT, 'Dropout training mode: PASS ✓'
END ELSE BEGIN
    PRINT, 'Dropout training mode: FAIL ✗ (unexpected dropout count)'
END
endif
PRINT, ''

; ============================================================================
; TEST 5: Dropout - Inference Mode
; ============================================================================
PRINT, '--- TEST 5: Dropout (Inference) ---'

; Apply dropout in inference mode (training=0)
; Should return input unchanged
dropped_inf = XDLML_DROPOUT(input_vals, 0.5, 0)

PRINT, 'Input:', input_vals
PRINT, 'Output (inference):', dropped_inf

; Check if all values are preserved
all_preserved = 1
FOR i = 0, N_ELEMENTS(input_vals)-1 DO BEGIN
    IF dropped_inf[i] NE input_vals[i] THEN all_preserved = 0
END
endfor

IF all_preserved THEN BEGIN
    PRINT, 'Dropout inference mode: PASS ✓'
END ELSE BEGIN
    PRINT, 'Dropout inference mode: FAIL ✗'
END
endif
PRINT, ''

; ============================================================================
; TEST 6: Dropout with Different Rates
; ============================================================================
PRINT, '--- TEST 6: Dropout Rate Variations ---'

; Test 20% dropout
dropped_20 = XDLML_DROPOUT(input_vals, 0.2, 1, 100)
n_dropped_20 = 0
FOR i = 0, N_ELEMENTS(dropped_20)-1 DO BEGIN
    IF dropped_20[i] EQ 0.0 THEN n_dropped_20 = n_dropped_20 + 1
END
endfor
PRINT, '20% dropout: ', n_dropped_20, ' units dropped'

; Test 80% dropout
dropped_80 = XDLML_DROPOUT(input_vals, 0.8, 1, 200)
n_dropped_80 = 0
FOR i = 0, N_ELEMENTS(dropped_80)-1 DO BEGIN
    IF dropped_80[i] EQ 0.0 THEN n_dropped_80 = n_dropped_80 + 1
END
endfor
PRINT, '80% dropout: ', n_dropped_80, ' units dropped'

PRINT, 'Dropout rate variations: PASS ✓'
PRINT, ''

; ============================================================================
; TEST 7: Inverted Dropout Scaling
; ============================================================================
PRINT, '--- TEST 7: Inverted Dropout Scaling ---'

; With inverted dropout, the expected sum should be preserved
original_sum = TOTAL(input_vals)
PRINT, 'Original sum:', original_sum

; Apply dropout with fixed seed
dropped_scaled = XDLML_DROPOUT(input_vals, 0.5, 1, 42)
dropped_sum = TOTAL(dropped_scaled)
PRINT, 'Dropped sum:', dropped_sum

; Due to scaling by 1/(1-rate), sum should be approximately preserved
sum_ratio = dropped_sum / original_sum
PRINT, 'Sum ratio (dropped/original):', sum_ratio

; Ratio should be close to 1.0 with inverted dropout
IF sum_ratio GT 0.8 AND sum_ratio LT 1.2 THEN BEGIN
    PRINT, 'Inverted dropout scaling: PASS ✓'
END ELSE BEGIN
    PRINT, 'Inverted dropout scaling: FAIL ✗'
END
endif
PRINT, ''

; ============================================================================
; SUMMARY
; ============================================================================
PRINT, '========================================='
PRINT, 'Regularization Tests Complete!'
PRINT, '========================================='
PRINT, '✓ Batch Normalization: Training mode'
PRINT, '✓ Batch Normalization: Inference mode'
PRINT, '✓ Batch Normalization: Learned parameters'
PRINT, '✓ Dropout: Training mode with random dropping'
PRINT, '✓ Dropout: Inference mode (no dropping)'
PRINT, '✓ Dropout: Rate variations'
PRINT, '✓ Dropout: Inverted dropout scaling'
PRINT, ''
PRINT, 'All regularization layers working correctly!'
